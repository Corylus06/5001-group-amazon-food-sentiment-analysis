{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom time import time\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"/kaggle/input/amazon-fine-food-reviews/Reviews.csv\"\ndf = pd.DataFrame()\ndf = pd.read_csv(path)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.sample(400000)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[['Text', 'Summary']]\ndf.dropna(axis=0, inplace=True)                    \ndf.drop_duplicates(subset=['Summary'], inplace=True)  \ndf.reset_index(drop=1, inplace=True)\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['Text'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['Summary'][0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nimport nltk\nimport re\n# nlp = spacy.load('en_core_web_sm')\n# nlp = spacy.load('en_core_web_lg')\nnltk.download('stopwords')\nnltk.download('punkt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = nltk.corpus.stopwords.words('english')\n# nlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\ncontraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"can not\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\npunctuations = '!\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(stop_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(contraction_mapping)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(punctuations)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cleansing_text(text):\n    text = text.lower() # Convert to lowercase\n    text = re.sub('<pre>.*?</pre>', '', text, flags = re.DOTALL)  # Remove HTML tags\n    text = re.sub('<code>.*?</code>', '', text, flags = re.DOTALL)\n    text = re.sub('<[^>]+>', '',text ,flags = re.DOTALL)\n    text = ' '.join([contraction_mapping[i] if i in contraction_mapping else i for i in text.split(\" \")]) # Contraction mapping \n    text = re.sub(r\"'s\\b\", \"\", text)  # Remove (‘s) \n    text = re.sub(\"[^a-zA-Z]\" ,\" \", text) # Remove punctuations and special characters\n    text = ' '.join([i for i in text.split() if i not in punctuations]) # Remove punctuations\n    text = ' '.join([i for i in text.split() if i not in stop_words]) # Remove stop_words\n#     text = ''.join([str(doc) for doc in nlp.pipe(text, batch_size = 5000, n_threads=-1)])\n    return text\n\ndef cleansing_summary(summary):\n    summary = summary.lower() # Convert to lowercase\n    summary = re.sub('<pre>.*?</pre>', '', summary, flags = re.DOTALL)  # Remove HTML tags\n    summary = re.sub('<code>.*?</code>', '', summary, flags = re.DOTALL)\n    summary = re.sub('<[^>]+>', '',summary ,flags = re.DOTALL)\n    summary = ' '.join([contraction_mapping[i] if i in contraction_mapping else i for i in summary.split(\" \")]) # Contraction mapping \n    summary = re.sub(r\"'s\\b\", \"\", summary)  # Remove (‘s) \n    summary = re.sub(\"[^a-zA-Z]\" ,\" \", summary) # Remove punctuations and special characters\n    summary = ' '.join([i for i in summary.split() if i not in punctuations]) # Remove personal punctuations\n    summary = ' '.join([i for i in summary.split() if i not in stop_words]) # Remove stop_words\n#     summary = ''.join([str(doc) for doc in nlp.pipe(summary, batch_size = 5000, n_threads=-1)])\n#     summary = 'START_ ' + str(summary) + ' END_'\n    return summary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm\n\ntexts = []\nfor text in tqdm(df['Text']):\n    texts.append(cleansing_text(text))\ndf['Text_Cleaned'] = texts  \nprint(\"::::: Text_Cleaned :::::\")\nprint(df['Text_Cleaned'][0:5], \"\\n\")\n\nsummaries = []\nfor text in tqdm(df['Summary']):\n    summaries.append(cleansing_summary(text))\ndf['Summary_Cleaned'] =  summaries \nprint(\"::::: Summary :::::\")\nprint(df['Summary_Cleaned'][0:5], \"\\n\")\n\ncorpus = list(df['Text_Cleaned'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['Text_Cleaned'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['Summary_Cleaned'][0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Preprocessing (Count word in Sentences)"},{"metadata":{"trusted":true},"cell_type":"code","source":"text_count = []\nsummary_count = []\n\nfor sent in df['Text_Cleaned']:\n    text_count.append(len(sent.split()))\nfor sent in df['Summary_Cleaned']:\n    summary_count.append(len(sent.split()))\n\ngraph_df = pd.DataFrame()\ngraph_df['text'] = text_count\ngraph_df['summary'] = summary_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"graph_df['text'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"graph_df['summary'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"graph_df['text'].hist(bins = 25, range=(0, 200))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"graph_df['summary'].hist(bins = 15, range=(0, 15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check how much % of text have 10-100 words\ncount = 0\nfor i in graph_df['text']:\n    if i > 10 and i <= 100:\n        count = count + 1\nprint(count / len(graph_df['text']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check how much % of summary have 2-10 words\ncount = 0\nfor i in graph_df['summary']:\n    if i > 1 and i <= 10:\n        count = count + 1\nprint(count / len(graph_df['summary']))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model to summarize  \n# 11 - 100 words for Text\n# 2 - 10 words for Summary \n\nmax_text_len = 100\nmax_summary_len = 10\n\ncleaned_text = np.array(df['Text_Cleaned'])\ncleaned_summary = np.array(df['Summary_Cleaned'])\n\nshort_text = []\nshort_summary = []\n\nfor i in range(len(cleaned_text)):\n    if(len(cleaned_summary[i].split()) <= max_summary_len \n       and len(cleaned_summary[i].split()) > 1 \n       and len(cleaned_text[i].split()) <= max_text_len \n       and len(cleaned_text[i].split()) > 10):\n        short_text.append(cleaned_text[i])\n        short_summary.append(cleaned_summary[i])\n        \npost_pre = pd.DataFrame({'text':short_text,'summary':short_summary})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add sostok and eostok\npost_pre['summary'] = post_pre['summary'].apply(lambda x : 'sostok '+ x + ' eostok')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"post_pre.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"post_pre","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Preprocessing (Train Test Split)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer \nfrom keras.preprocessing.sequence import pad_sequences\n\n# train test split\nx_tr,x_test,y_tr,y_test = train_test_split(np.array(post_pre['text']),\n                                         np.array(post_pre['summary']),\n                                         test_size = 0.2,\n                                         random_state = 0,\n                                         shuffle = True)\n# train validation split\nx_tr,x_val,y_tr,y_val = train_test_split(x_tr,\n                                         y_tr,\n                                         test_size = 0.2,\n                                         random_state = 0,\n                                         shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_tr.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_val.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Preprocessing (Rare Word Analysis)\ntot_cnt = Size of vocabulary (unique words in the text)\n\ncnt = No. of rare words whose count falls below threshold\n\ntot_cnt - cnt = The top most common words"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenize text to get the vocab count\n#prepare a tokenizer for training data\nx_tokenizer = Tokenizer() \nx_tokenizer.fit_on_texts(list(x_tr))\n\n#prepare a tokenizer for reviews on training data\ny_tokenizer = Tokenizer()   \ny_tokenizer.fit_on_texts(list(y_tr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"thresh = 6\ncnt = 0\n# tot_cnt = 0\ntot_cnt = len(x_tokenizer.word_counts)\nfreq = 0\ntot_freq = 0\n\nkeys = []\nvalues = []\n\nfor key,value in x_tokenizer.word_counts.items():\n    keys.append(key)\n    values.append(value)\n    if(value < thresh):\n        cnt = cnt + 1\n\ndf_frequency = pd.DataFrame({'word':keys,'frequency':values})\ndf_frequency.sort_values(by='frequency', ascending=False, inplace=True)\ndf_frequency.reset_index(inplace=True, drop=0)\ndf_frequency","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"% Rare words in vocabulary:\",(cnt / tot_cnt) * 100)\ntot_cnt, cnt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(6,10), ncols=1, nrows=1)\nsns.barplot(x='frequency',y='word',data=df_frequency[:20], palette='Reds_r', ax=ax);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prepare a tokenizer for reviews on training data\nx_tokenizer = Tokenizer(num_words = tot_cnt - cnt) \nx_tokenizer.fit_on_texts(list(x_tr))\n\n#convert text sequences into integer sequences (i.e one-hot encodeing all the words)\nx_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \nx_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\nx_test_seq = x_tokenizer.texts_to_sequences(x_test)\n\n#padding zero upto maximum length\nx_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\nx_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\nx_test = pad_sequences(x_test_seq, maxlen=max_text_len, padding='post')\n\n#size of vocabulary ( +1 for padding token)\nx_voc   =  x_tokenizer.num_words + 1\n\nprint(\"Size of vocabulary in X = {}\".format(x_voc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"thresh = 3\ncnt = 0\ntot_cnt = len(y_tokenizer.word_counts)\nfreq = 0\ntot_freq = 0\n\nkeys = []\nvalues = []\n\nfor key,value in y_tokenizer.word_counts.items():\n    keys.append(key)\n    values.append(value)\n    if(value < thresh):\n        cnt = cnt + 1\n\ndf_frequency = pd.DataFrame({'word':keys,'frequency':values})\ndf_frequency.sort_values(by='frequency', ascending=False, inplace=True)\ndf_frequency.reset_index(inplace=True, drop=0)\ndf_frequency","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"% Rare words in vocabulary:\",(cnt / tot_cnt) * 100)\ntot_cnt, cnt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"% Rare words in vocabulary:\",(cnt / tot_cnt) * 100)\ntot_cnt, cnt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(6,10), ncols=1, nrows=1)\nsns.barplot(x='frequency',y='word',data=df_frequency[3:20], palette='Reds_r', ax=ax);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prepare a tokenizer for reviews on training data\ny_tokenizer = Tokenizer(num_words = tot_cnt-cnt) \ny_tokenizer.fit_on_texts(list(y_tr))\n\n#convert text sequences into integer sequences (i.e one hot encode the text in Y)\ny_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \ny_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \ny_test_seq = y_tokenizer.texts_to_sequences(y_test) \n\n#padding zero upto maximum length\ny_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\ny_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\ny_test = pad_sequences(y_test_seq, maxlen=max_summary_len, padding='post')\n\n#size of vocabulary\ny_voc  =   y_tokenizer.num_words +1\nprint(\"Size of vocabulary in Y = {}\".format(y_voc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.backend import clear_session\nimport gensim\nfrom numpy import *\nimport numpy as np\nimport pandas as pd \nimport re\nfrom bs4 import BeautifulSoup\nfrom keras.preprocessing.text import Tokenizer \nfrom keras.preprocessing.sequence import pad_sequences\nfrom nltk.corpus import stopwords\nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Size of vocabulary from the w2v model = {x_voc}\")\n\nclear_session()\n\nlatent_dim = 256\nembedding_dim = 128\n\n# Encoder\nencoder_inputs = Input(shape=(max_text_len,))\n\n#embedding layer\nenc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n\n#encoder lstm 1\nencoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\nencoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n\n#encoder lstm 2\nencoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\nencoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n\n#encoder lstm 3\nencoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\nencoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n\n# Set up the decoder, using `encoder_states` as initial state.\ndecoder_inputs = Input(shape=(None,))\n\n#embedding layer\ndec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\ndec_emb = dec_emb_layer(decoder_inputs)\n\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\ndecoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n\n#dense layer\ndecoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n# Define the model \nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)\n\nhistory=model.fit([x_tr,y_tr[:,:-1]], \n                  y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:],\n                  epochs=8,\n                  callbacks=[es],\n                  batch_size=128, \n                  validation_data=([x_val,y_val[:,:-1]], \n                                   y_val.reshape(y_val.shape[0],\n                                                 y_val.shape[1], \n                                                 1)[:,1:]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making Summaries"},{"metadata":{"trusted":true},"cell_type":"code","source":"reverse_target_word_index=y_tokenizer.index_word\nreverse_source_word_index=x_tokenizer.index_word\ntarget_word_index=y_tokenizer.word_index\n\n# Encode the input sequence to get the feature vector\nencoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n\n# Decoder setup\n# Below tensors will hold the states of the previous time step\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n\n# Get the embeddings of the decoder sequence\ndec_emb2= dec_emb_layer(decoder_inputs) \n# To predict the next word in the sequence, set the initial states to the states from the previous time step\ndecoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n\n# A dense softmax layer to generate prob dist. over the target vocabulary\ndecoder_outputs2 = decoder_dense(decoder_outputs2) \n\n# Final decoder model\ndecoder_model = Model(\n    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n    [decoder_outputs2] + [state_h2, state_c2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    e_out, e_h, e_c = encoder_model.predict(input_seq)\n    \n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1,1))\n    \n    # Populate the first word of target sequence with the start word.\n    target_seq[0, 0] = target_word_index['sostok']\n\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n      \n        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_token = reverse_target_word_index[sampled_token_index]\n        \n        if(sampled_token!='eostok'):\n            decoded_sentence += ' '+sampled_token\n\n        # Exit condition: either hit max length or find stop word.\n        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1,1))\n        target_seq[0, 0] = sampled_token_index\n\n        # Update internal states\n        e_h, e_c = h, c\n\n    return decoded_sentence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seq2summary(input_seq):\n    newString=''\n    for i in input_seq:\n        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n            newString=newString+reverse_target_word_index[i]+' '\n    return newString\n\ndef seq2text(input_seq):\n    newString=''\n    for i in input_seq:\n        if(i!=0):\n            newString=newString+reverse_source_word_index[i]+' '\n    return newString","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"../input/5001-output/Outputdata\"\nDF = pd.DataFrame()\nDF = pd.read_csv(path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DF.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# texts2 = []\n# for text in tqdm(DF['0']):\n#     texts2.append(cleansing_text(text))\n# DF['Text_Cleaned'] = texts2  \n# print(\"::::: Text_Cleaned :::::\")\n# print(DF['Text_Cleaned'][0:5], \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert text sequences into integer sequences (i.e one-hot encodeing all the words)\n# z_tr_seq    =   x_tokenizer.texts_to_sequences(DF['Text_Cleaned']) \n\n#padding zero upto maximum length\n# z_tr    =   pad_sequences(z_tr_seq,  maxlen=max_text_len, padding='post')\n\n#size of vocabulary ( +1 for padding token)\n# z_voc   =  x_tokenizer.num_words + 1\n\n# reverse_source_word_index=z_tokenizer.index_word","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"totle = []\ntotle_predicted = []\naccuracy = []\nsummary_pred = []\n\n# sample 5000 test\n# for i in tqdm(range(0, DF['Text_Cleaned'].shape[0])):\n#     review = seq2text(z_tr[i])\n#     original_summary = seq2summary(y_test[i])\n#     predicted_summary = decode_sequence(z_tr[i].reshape(1, max_text_len))\n#     summary_pred.append(predicted_summary)\n#     print(\"Review:\", review)\n#     print(\"Original summary:\", original_summary)\n#     print(\"Predicted summary:\", predicted_summary)\n    \n#     if len(original_summary.split()) != 0:\n#     count = 0\n#     for j in predicted_summary.split():\n#         if j in review:\n#             count += 1\n#     count = 0\n#     for k in decode_sequence(x_tr[i].reshape(1, max_text_len)).split():\n#         if k in original_summary:\n#             count += 1\n#     totle.append(len(predicted_summary.split()))\n#     accuracy.append(count/len(predicted_summary.split()))\n#     print(f\"{count} / {len(predicted_summary.split())}\")\n#     print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DF['summary_pred'] = summary_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sum(accuracy)/len(accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DF.to_csv('DF_output')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DF.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# score_1 = DF[DF['2'] < 0.5]\n# score_2 = DF[DF['2'] > 0.5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reviews_sample = pd.concat([score_1,score_2],axis=0)\n# reviews_sample.reset_index(drop=True,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import all library\nimport json\nimport nltk\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom nltk.probability import FreqDist\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import cm\nimport numpy as np\nimport string\nfrom wordcloud import WordCloud\nimport pickle\nimport re, string\nimport sys\nimport seaborn as sns\nsns.set()\n#Wordcloud function's input needs to be a single string of text.\n# concatenating all Summaries into a single string.\n# similarly you can build for Text column\n# reviews_str = reviews_sample.summary_pred.str.cat()\n# wordcloud = WordCloud(background_color='black').generate(reviews_str)\n# plt.figure(figsize=(10,10))\n# plt.imshow(wordcloud,interpolation='bilinear')\n# plt.axis(\"off\")\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data into positive & negative value\n# negative = reviews_sample[reviews_sample['2'].isin([0,0.5]) ]\n# positive = reviews_sample[reviews_sample['2'].isin([0.5,1]) ]\n# Transform to single string\n# negative_str = negative.summary_pred.str.cat()\n# positive_str = positive.summary_pred.str.cat()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# wordcloud_negative = WordCloud(background_color='white').generate(negative_str)\n# wordcloud_positive = WordCloud(background_color='white').generate(positive_str)\n# Plot negative review\n# fig = plt.figure(figsize=(10,10))\n# ax1 = fig.add_subplot(211)\n# ax1.imshow(wordcloud_negative,interpolation='bilinear')\n# ax1.axis(\"off\")\n# ax1.set_title('Negative Reviews',fontsize=25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"/kaggle/input/amazon-fine-food-reviews/Reviews.csv\"\nDF2 = pd.DataFrame()\nDF2 = pd.read_csv(path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DF2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texts3 = []\nfor text in tqdm(DF2['Text']):\n    texts3.append(cleansing_text(text))\nDF2['Text_Cleaned'] = texts3  \nprint(\"::::: Text_Cleaned :::::\")\nprint(DF2['Text_Cleaned'][0:5], \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert text sequences into integer sequences (i.e one-hot encodeing all the words)\nw_tr_seq    =   x_tokenizer.texts_to_sequences(DF2['Text_Cleaned']) \n\n#padding zero upto maximum length\nw_tr    =   pad_sequences(w_tr_seq,  maxlen=max_text_len, padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"totle = []\ntotle_predicted = []\naccuracy = []\nsummary_pred = []\n\n# sample 5000 test\nfor i in tqdm(range(0, 50000)):\n    review = seq2text(w_tr[i])\n#     original_summary = seq2summary(y_test[i])\n    predicted_summary = decode_sequence(w_tr[i].reshape(1, max_text_len))\n    summary_pred.append(predicted_summary)\n    print(\"Review:\", review)\n#     print(\"Original summary:\", original_summary)\n    print(\"Predicted summary:\", predicted_summary)\n    \n#     if len(original_summary.split()) != 0:\n#     count = 0\n#     for j in predicted_summary.split():\n#         if j in review:\n#             count += 1\n#     count = 0\n#     for k in decode_sequence(x_tr[i].reshape(1, max_text_len)).split():\n#         if k in original_summary:\n#             count += 1\n#     totle.append(len(predicted_summary.split()))\n#     accuracy.append(count/len(predicted_summary.split()))\n#     print(f\"{count} / {len(predicted_summary.split())}\")\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = pd.DataFrame()\ndf2['summary_pred'] = summary_pred\ndf2.to_csv('df_output')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"/kaggle/input/amazon-fine-food-reviews/Reviews.csv\"\nDF3 = pd.DataFrame()\nDF3 = pd.read_csv(path)\nDF3 = DF3[0:50000]\nDF3['summary_pred'] = summary_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_1 = DF3[DF3['Score'] ==1 ]\nscore_2 = DF3[DF3['Score'] ==2 ]\nscore_3 = DF3[DF3['Score'] ==3 ]\nscore_4 = DF3[DF3['Score'] ==4 ]\nscore_5 = DF3[DF3['Score'] ==5 ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews_sample = pd.concat([score_1,score_2, score_3, score_4, score_5],axis=0)\nreviews_sample.reset_index(drop=True,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews_str = reviews_sample.summary_pred.str.cat()\nwordcloud = WordCloud(background_color='black').generate(reviews_str)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud,interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data into positive & negative value\nnegative = reviews_sample[reviews_sample['Score'].isin([1,2]) ]\npositive = reviews_sample[reviews_sample['Score'].isin([4,5]) ]\n# Transform to single string\nnegative_str = negative.summary_pred.str.cat()\npositive_str = positive.summary_pred.str.cat()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud_negative = WordCloud(background_color='white').generate(negative_str)\nwordcloud_positive = WordCloud(background_color='white').generate(positive_str)\n# Plot negative review\nfig = plt.figure(figsize=(10,10))\nax1 = fig.add_subplot(211)\nax1.imshow(wordcloud_negative,interpolation='bilinear')\nax1.axis(\"off\")\nax1.set_title('Negative Reviews',fontsize=25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_tokenizer = Tokenizer() \nf_tokenizer.fit_on_texts(list(DF3['summary_pred']))\nthresh = 6\ncnt = 0\n# tot_cnt = 0\ntot_cnt = len(f_tokenizer.word_counts)\nfreq = 0\ntot_freq = 0\n\nkeys = []\nvalues = []\n\nfor key,value in f_tokenizer.word_counts.items():\n    keys.append(key)\n    values.append(value)\n    if(value < thresh):\n        cnt = cnt + 1\n\ndf_frequency = pd.DataFrame({'word':keys,'frequency':values})\ndf_frequency.sort_values(by='frequency', ascending=False, inplace=True)\ndf_frequency.reset_index(inplace=True, drop=0)\ndf_frequency\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}